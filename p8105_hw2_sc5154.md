p8105_hw2_sc5154
================
Shaohan Chen
2022-10-02

Load necessary packages.

``` r
library(tidyverse)
library(janitor)
library(readxl)
```

## Problem 1

The row data for NYC transit subway is:

``` r
subway_raw_data = read_csv("Data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv")
```

First we read and clean the data from the local file. And also retain
the columns mentioned in the problem.

``` r
subway_data = 
  read_csv("Data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names(dat = .) %>%
  select(.data = ., line:entry, vending, ada)
```

Next I conduct some further cleaning.

Remove the rows and columns that are all `NA`s.

``` r
subway_data = 
  subway_data %>%
    remove_empty(., whic = c("rows")) %>%
    remove_empty(., whic = c("cols")) 
```

And I figure out that the route 8-11 have different data types from
route 1-7. After checking specific data, it is because the route 8-11 is
named by number. In order to avoid mistakes in further process like
`pivot_longer`, I transform the data type of route 8-11 into characters.

``` r
subway_data$route8 = as.character(subway_data$route8)
subway_data$route9 = as.character(subway_data$route9)
subway_data$route10 = as.character(subway_data$route10)
subway_data$route11 = as.character(subway_data$route11)
```

Convert the entry variable from character to a logical variable

``` r
subway_data$entry = recode(subway_data$entry, 'YES' = TRUE, 'NO' = FALSE)
```

The raw dataset contains 32 columns before my cleaning and tailor, which
include various detailed descriptive elements of the NYC transit subway
system, including the line index, station name, station size, the routes
operated in the station, the entrance information, ada, station location
and entrance location, etc.

So far my data cleaning steps include: 1. `janitor::clean_names` to
clean the column names.

2.  Remove the row and columns that are all `NA`s.

3.  Transform the types of route8-11 into characters for consistency.

4.  Convert the `entry` column from ‘YES’ or ‘NO’ to the logical
    variable.

The dimension of the resulting dataset is totally 1868 rows and 19
columns.

These data are not so tidy. The columns `route1` to `route11` contain
too much `NA` values, which makes the dataset too wide. Route names and
route numbers should be variables.

``` r
subway_data %>% 
  select(station_name, line) %>% 
  distinct %>%
  nrow
## [1] 465
```

There are 465 distinct stations.

``` r
subway_data %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct %>%
  nrow
## [1] 84
```

There are 84 stations that are ADA complaint.

``` r
subway_data %>%
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
## [1] 0.3770492
```

The proportion of station entrances/exits without vending allow entrance
is 0.3770.

Next I reformat the data to make route number and route name be distinct
variables.

``` r
subway_tidy_data = 
  subway_data %>% 
    pivot_longer(
      route1:route11,
      names_to = "route_index",
      values_to = "route_name") 
```

Count the number of distinct stations that serve the A train.

``` r
subway_tidy_data %>%
  filter(route_name == "A") %>% 
  select(station_name, line) %>% 
  distinct %>%
  nrow
## [1] 60
```

Therefore, there are 60 distinct stations serve the A train.

``` r
subway_tidy_data %>%
  filter(route_name == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct %>%
  nrow
## [1] 17
```

Therefore, there are 17 distinct stations serve the A train and are ADA
complaint.

## Problem 2

Read and clean the Mr.Trash Wheel dataset.

1.  Specify the sheet and omit non-data entries including the first row
    that contains figures and notes, and omit the columns from `O` that
    contain notes or filled with spaces using arguments in
    `read_excel()`.

2.  Clean the column names and rename some variables to make them more
    reasonable using `janitor::clean_names` and `rename()`.

3.  Omit the rows that do not include specific dumpster or time data
    using `drop_na()`.

4.  Round the number of sports balls to the nearest integer and converts
    the result to an integer variable using `round()` and `as.integer`.

``` r
mr_trash_data = 
  read_excel("Data/Trash Wheel Collection Data.xlsx", sheet = 1, skip = 1, range = cell_cols("A:N")) %>%
  janitor::clean_names(dat = .) %>%
  rename(.data = ., weight = weight_tons, volume = volume_cubic_yards) %>%
  drop_na(dumpster, year, date) %>%
  mutate(.data = ., sports_balls = as.integer((round(sports_balls))))
```

And we repeat similar steps towards Professor Trash Wheel dataset.

``` r
professor_trash_data = 
  read_excel("Data/Trash Wheel Collection Data.xlsx", sheet = 2, skip = 1, range = cell_cols("A:M")) %>%
  janitor::clean_names(dat = .) %>%
  rename(.data = ., weight = weight_tons, volume = volume_cubic_yards) %>%
  drop_na(dumpster, year, date) %>%
  mutate(.data = ., dumpster = as.double(dumpster), year = as.character(year),  sports_balls = NA)
```

Then combine those two datasets to produce a single tidy dataset using
`bind_rows`, and add an additional variable indicating which Trash Wheel
is which to both datasets before combining.

``` r
mr_trash_data  = 
  mr_trash_data %>% 
  mutate(.data = ., trash_wheel_type = "mr_trash_wheel")
professor_trash_data =
  professor_trash_data %>%
  mutate(.data = ., trash_wheel_type = "professor_trash_wheel") 
combine_trash_data = bind_rows(mr_trash_data, professor_trash_data)
```

Write a paragraph about these data:

The two separate datasets respectively record the information of trash
collected from two different wheels called “Mr. Trash Wheel” and
“Professor Trash Wheel”. Those two datasets have same variables and the
variable data types. It can been seen from the data that Professor Trash
Wheel operated later than Mr. Trash Wheel.

The combined dataset has 641 rows and 15 columns, with 547 rows of data
from Mr. Trash Wheel and 94 rows of data from Professor Trash Wheel.

The combined dataset’s variables include the dumpster index, collected
time (day/month/year), trash weight, trash volume, trash amount on
different kinds(plastic bottle, polystyrene, etc.), the power for homes
generated by trash, and the manually added trash wheel type.

The total trash weight collected by Mr. Trash Wheel is 1748.36. And the
total weight each year from 2014 to 2020 is: 141.35, 238.8, 164.59,
174.84, 310.39, 200.24, 216.68.

The total trash weight collected by Professor Trash Wheel is 190.12.

And the total weight each year from 2017 to 2020 is: 36.57, 43.81,
30.04, 23.4.

The total trash volume collected by Mr. Trash Wheel is 8385.  
The total trash volume collected by Professor Trash Wheel is 1371.

The total homes powered by Mr. Trash Wheel is 2.2344^{4}. The total
homes powered by Professor Trash Wheel is 2709.

For the last question in problem 2:

``` r
total_weight_professor = 
  combine_trash_data %>%
  filter(trash_wheel_type == "professor_trash_wheel") %>%
  pull(weight) %>%
  sum
```

``` r
total_number_balls_2020 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel", year == "2020") %>%
  pull(sports_balls) %>%
  na.omit() %>%
  sum
```

The total weight of trash collected by Professor Trash Wheel is 190.12.
The total number of sports balls collected by Mr. Trash Wheel is 856.

## Problem 3

First clean the data in pols-month.csv. Use `separate()` to break up the
`mon`. Replace the month number with month name using `month.name`.
Create a `president` variable taking values from `gop` and `dem`, and
remove the variable `prez_dem` and `prez_gop`, and `day`. (Note that the
data in column `prez_gop` appears value `2` which is not in the
description of the dataset, it may be a mistake within the dataset.)

``` r
pols_month_data = 
  read_csv("Data/fivethirtyeight_datasets/pols-month.csv") %>%
  separate(data = ., mon, c("year", "month", "day"), sep = "-") %>%
  arrange(year, month) %>%
  mutate(.data = ., year = as.integer(year), month = month.name[as.integer(month)], day = as.integer(day), president = recode(prez_dem, "1" = "dem", "0" = "gop")) %>% 
  select(-prez_dem, -prez_gop, -day)
```

Next read and clean the data in snp.csv using a similar process. Note
that we need to take care on the `year` variable. Arrange the dataset
according to year and month, and organize so that those two variables
are leading columns.

``` r
snp_data = 
  read_csv("Data/fivethirtyeight_datasets/snp.csv") %>%
  separate(data = ., date, c("month", "day", "year"), sep = "/") %>%
  mutate(month = as.numeric(month), year = as.integer(year), year = ifelse((year >= 50), year + 1900,
year + 2000),) %>%
  arrange(year, month) %>%
  mutate(.data = ., month = month.name[as.integer(month)], day = as.integer(day)) %>% 
  select(year, month, close) 
```

Next, read and tidy the unemployment data for further merging process.
And make sure the key variables have the same name and take the same
values.

``` r
unemployement_data = 
  read_csv("Data/fivethirtyeight_datasets/unemployment.csv") %>%
  rename(year = Year) %>%
  pivot_longer(
    data = .,
    Jan:Dec,
    names_to = "month",
    values_to = "unemployment"
  ) %>%
  mutate(month = recode(month,
  'Jan' = 'January',
  'Feb' = 'February',
  'Mar' = 'March',
  'Apr' = 'April',
  'Jun' = 'June',
  'Jul' = 'July',
  'Aug' = 'August',
  'Sep' = 'September',
  'Oct' = 'October',
  'Nov' = 'November',
  'Dec' = 'December')) %>%
  mutate(.data = ., year = as.integer(year), month = as.character(month))
```

Join the datasets altogether.

``` r
temp_merge_data =
  left_join(pols_month_data, snp_data, by = c("year" = "year", "month" = "month"))
result_ljoin_fte_data = 
  left_join(temp_merge_data, unemployement_data, by = c("year" = "year", "month" = "month"))
```

The resulting dataset using `left_join` has some `NA`s in it.

The result dataset by removing `NA`s can be computed:

``` r
result_fte_data = 
  result_ljoin_fte_data %>%
  na.omit()
```

(Note: we can also use `merge` function after Google, as shows below:)

``` r
result_merge_fte_data =
  merge(pols_month_data, snp_data) %>%
  merge(., unemployement_data)
```

Write a short paragraph about these datasets:

The first dataset pols_month_data contain 822 observations of 9
variables related to the number of national politicians(president,
governor, senator, representative) who are democratic or republican at
any given time.

The second dataset snp_data contains 787 observations of 3 variables
related to Standard & Poor’s stock market index, and the `close`
variable refers to the closing values of the stock index on the
associated date.

The third dataset unemployment_data contains 816 observations of 3
variables which indicate the percentage of unemployment in every month
of different associated years.

The merged dataset result_fte_data contains 786 observations of 11
variables, which merged the recorded variables from 3 separate datasets
above, and provides a straightforward view of the whole data from
multiple aspects, including key metrics like time(year/month),
president, close and unemployment. The range of years recorded in this
dataset is 65.

A comparison of unemployment rate between when president is republican
and democratic can be made.

The average unemployment rate when the president is republican is
5.9310185, which is slightly higher than the average unemployment rate
when the president is democratic that is 5.769774.

And meanwhile, the close index when the president is republican is
378.2738635, which is lower than when the president is democratic that
is 588.2584173.

It is interesting since it looks like the economic state seems to be
better when the president is democratic. But this statement is not too
rigorous. To promote this finding and draw a clear conclusion, we still
need additional data support and further investigation.

Thanks for your time!
