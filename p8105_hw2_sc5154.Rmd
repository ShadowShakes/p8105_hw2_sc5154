---
title: "p8105_hw2_sc5154"
author: "Shaohan Chen"
date: "2022-10-02"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```

Load necessary packages.
```{r}
library(tidyverse)
library(janitor)
library(readxl)
```

## Problem 1

The row data for NYC transit subway is:

```{r, results = "hide"}
subway_raw_data = read_csv("Data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv")
```

First we read and clean the data from the local file. And also retain the columns mentioned in the problem.

```{r}
subway_data = 
  read_csv("Data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names(dat = .) %>%
  select(.data = ., line:entry, vending, ada)
```

Next I conduct some further cleaning.

Remove the rows and columns that are all `NA`s.

```{r}
subway_data = 
  subway_data %>%
    remove_empty(., whic = c("rows")) %>%
    remove_empty(., whic = c("cols")) 
```

And I figure out that the route 8-11 have different data types from route 1-7. After checking specific data, it is because the route 8-11 is named by number. In order to avoid mistakes in further process like `pivot_longer`, I transform the data type of route 8-11 into characters.

```{r}
subway_data$route8 = as.character(subway_data$route8)
subway_data$route9 = as.character(subway_data$route9)
subway_data$route10 = as.character(subway_data$route10)
subway_data$route11 = as.character(subway_data$route11)
```

Convert the entry variable from character to a logical variable

```{r}
subway_data$entry = recode(subway_data$entry, 'YES' = TRUE, 'NO' = FALSE)
```

The raw dataset contains 32 columns before my cleaning and tailor, which include various detailed descriptive elements of the NYC transit subway system, including the line index, station name, station size, the routes operated in the station, the entrance information, ada, station location and entrance location, etc.

So far my data cleaning steps include: 
1. `janitor::clean_names` to clean the column names.

2. Remove the row and columns that are all `NA`s.

3. Transform the types of route8-11 into characters for consistency.

4. Convert the `entry` column from 'YES' or 'NO' to the logical variable. 


```{r echo = FALSE}
subway_data_rows = nrow(subway_data)
subway_data_cols = ncol(subway_data)
```

The dimension of the resulting dataset is totally `r subway_data_rows` rows and `r subway_data_cols` columns.

These data are not so tidy. The columns `route1` to `route11` contain too much `NA` values, which makes the dataset too wide. Route names and route numbers should be variables.

```{r}
subway_data %>% 
  select(station_name, line) %>% 
  distinct %>%
  nrow
```

There are 465 distinct stations.

```{r}
subway_data %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct %>%
  nrow
```

There are 84 stations that are ADA complaint.

```{r}
subway_data %>%
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
```

The proportion of station entrances/exits without vending allow entrance is 0.3770.

Next I reformat the data to make route number and route name be distinct variables.
```{r}
subway_tidy_data = 
  subway_data %>% 
    pivot_longer(
      route1:route11,
      names_to = "route_index",
      values_to = "route_name") 
```

Count the number of distinct stations that serve the A train.
```{r}
subway_tidy_data %>%
  filter(route_name == "A") %>% 
  select(station_name, line) %>% 
  distinct %>%
  nrow
```
Therefore, there are 60 distinct stations serve the A train.

```{r}
subway_tidy_data %>%
  filter(route_name == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct %>%
  nrow
```
Therefore, there are 17 distinct stations serve the A train and are ADA complaint.



## Problem 2

Read and clean the Mr.Trash Wheel dataset.

1. Specify the sheet and omit non-data entries including the first row that contains figures and notes, and omit the columns from `O` that contain notes or filled with spaces using arguments in `read_excel()`.

2. Clean the column names and rename some variables to make them more reasonable using `janitor::clean_names` and `rename()`.

3. Omit the rows that do not include specific dumpster or time data using `drop_na()`.

4. Round the number of sports balls to the nearest integer and converts the result to an integer variable using `round()` and `as.integer`.
```{r}
mr_trash_data = 
  read_excel("Data/Trash-Wheel-Collection-Totals-7-2020-2.xlsx", sheet = 1, skip = 1, range = cell_cols("A:N")) %>%
  janitor::clean_names(dat = .) %>%
  rename(.data = ., weight = weight_tons, volume = volume_cubic_yards) %>%
  drop_na(dumpster, year, date) %>%
  mutate(.data = ., sports_balls = as.integer((round(sports_balls))))
```


And we repeat similar steps towards Professor Trash Wheel dataset.
```{r}
professor_trash_data = 
  read_excel("Data/Trash-Wheel-Collection-Totals-7-2020-2.xlsx", sheet = 2, skip = 1, range = cell_cols("A:N")) %>%
  janitor::clean_names(dat = .) %>%
  rename(.data = ., weight = weight_tons, volume = volume_cubic_yards) %>%
  drop_na(dumpster, year, date) %>%
  mutate(.data = ., sports_balls = as.integer((round(sports_balls))), dumpster = as.character(dumpster))
```

Then combine those two datasets to produce a single tidy dataset using `bind_rows`, and add an additional variable indicating which Trash Wheel is which to both datasets before combining.
```{r}
mr_trash_data  = 
  mr_trash_data %>% 
  mutate(.data = ., trash_wheel_type = "mr_trash_wheel")
professor_trash_data =
  professor_trash_data %>%
  mutate(.data = ., trash_wheel_type = "professor_trash_wheel") 
combine_trash_data = bind_rows(mr_trash_data, professor_trash_data)
```


```{r echo = FALSE}
com_row = nrow(combine_trash_data)
com_col = ncol(combine_trash_data)

mr_row = nrow(mr_trash_data)
pro_row = nrow(professor_trash_data)

total_weight_mr = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel") %>% 
  pull(weight) %>% 
  sum

total_weight_mr_2014 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel", year == "2014") %>% 
  pull(weight) %>% 
  sum

total_weight_mr_2015 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel", year == "2015") %>% 
  pull(weight) %>% 
  sum

total_weight_mr_2016 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel", year == "2016") %>% 
  pull(weight) %>% 
  sum

total_weight_mr_2017 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel", year == "2017") %>% 
  pull(weight) %>% 
  sum

total_weight_mr_2018 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel", year == "2018") %>% 
  pull(weight) %>% 
  sum

total_weight_mr_2019 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel", year == "2019") %>% 
  pull(weight) %>% 
  sum

total_weight_mr_2020 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel", year == "2020") %>% 
  pull(weight) %>% 
  sum

total_weight_pro = 
  combine_trash_data %>%
  filter(trash_wheel_type == "professor_trash_wheel") %>% 
  pull(weight) %>% 
  sum

total_weight_pro_2017 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "professor_trash_wheel", year == "2017") %>% 
  pull(weight) %>% 
  sum

total_weight_pro_2018 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "professor_trash_wheel", year == "2018") %>% 
  pull(weight) %>% 
  sum

total_weight_pro_2019 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "professor_trash_wheel", year == "2019") %>% 
  pull(weight) %>% 
  sum

total_weight_pro_2020 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "professor_trash_wheel", year == "2020") %>% 
  pull(weight) %>% 
  sum

total_volumn_mr = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel") %>% 
  pull(volume) %>% 
  sum

total_volumn_pro = 
  combine_trash_data %>%
  filter(trash_wheel_type == "professor_trash_wheel") %>% 
  pull(volume) %>% 
  sum

total_power_mr = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel") %>% 
  pull(homes_powered) %>% 
  sum %>%
  round

total_power_pro = 
  combine_trash_data %>%
  filter(trash_wheel_type == "professor_trash_wheel") %>% 
  pull(homes_powered) %>% 
  sum %>%
  round
```


Write a paragraph about these data:

The two separate datasets respectively record the information of trash collected from two different wheels called "Mr. Trash Wheel" and "Professor Trash Wheel". Those two datasets have same variables and the variable data types. It can been seen from the data that Professor Trash Wheel operated later than Mr. Trash Wheel.  

The combined dataset has `r com_row` rows and `r com_col` columns, with `r mr_row` rows of data from Mr. Trash Wheel and `r pro_row` of data from Professor Trash Wheel.

The combined dataset's variables include the dumpster index, collected time (day/month/year), trash weight, trash volume, trash amount on different kinds(plastic bottle, polystyrene, etc.), the power for homes generated by trash, and the manually added trash wheel type.


The total trash weight collected by Mr. Trash Wheel is `r total_weight_mr`.
And the total weight each year from 2014 to 2020 is:
`r total_weight_mr_2014`, `r total_weight_mr_2015`, `r total_weight_mr_2016`, `r total_weight_mr_2017`, `r total_weight_mr_2018`, `r total_weight_mr_2019`, `r total_weight_mr_2020`.

The total trash weight collected by Professor Trash Wheel is `r total_weight_pro`.

And the total weight each year from 2017 to 2020 is:
`r total_weight_pro_2017`, `r total_weight_pro_2018`, `r total_weight_pro_2019`, `r total_weight_pro_2020`.

The total trash volume collected by Mr. Trash Wheel is `r total_volumn_mr`.    
The total trash volume collected by Professor Trash Wheel is `r total_volumn_pro`.

The total homes powered by Mr. Trash Wheel is `r total_power_mr`.
The total homes powered by Professor Trash Wheel is `r total_power_pro`.

For the last question in problem 2:
```{r}
total_weight_professor = 
  combine_trash_data %>%
  filter(trash_wheel_type == "professor_trash_wheel") %>%
  pull(weight) %>%
  sum
```

```{r}
total_number_balls_2020 = 
  combine_trash_data %>%
  filter(trash_wheel_type == "mr_trash_wheel", year == "2020") %>%
  pull(sports_balls) %>%
  sum
```
The total weight of trash collected by Professor Trash Wheel is `r total_weight_professor`.
The total number of sports balls collected by Mr. Trash Wheel is `r total_number_balls_2020`.



## Problem 3


First clean the data in pols-month.csv. Use `separate()` to break up the `mon`. Replace the month number with month name using `month.name`. Create a `president` variable taking values from `gop` and `dem`, and remove the variable `prez_dem` and `prez_gop`, and `day`.
(Note that the data in column `prez_gop` appears value `2` which is not in the description of the dataset, it may be a mistake within the dataset.)
```{r}
pols_month_data = 
  read_csv("Data/fivethirtyeight_datasets/pols-month.csv") %>%
  separate(data = ., mon, c("year", "month", "day"), sep = "-") %>%
  arrange(year, month) %>%
  mutate(.data = ., year = as.integer(year), month = month.name[as.integer(month)], day = as.integer(day), president = recode(prez_dem, "1" = "dem", "0" = "gop")) %>% 
  select(-prez_dem, -prez_gop, -day)
```

Next read and clean the data in snp.csv using a similar process. Note that we need to take care on the `year` variable.
Arrange the dataset according to year and month, and organize so that those two variables are leading columns.

```{r}
snp_data = 
  read_csv("Data/fivethirtyeight_datasets/snp.csv") %>%
  separate(data = ., date, c("month", "day", "year"), sep = "/") %>%
  mutate(month = as.numeric(month)) %>%
  select(year, month, close) %>%
  arrange(year, month)
```

```{r}
snp_data = 
  read_csv("Data/fivethirtyeight_datasets/snp.csv") %>%
  separate(data = ., date, c("month", "day", "year"), sep = "/") %>%
  mutate(month = as.numeric(month), year = as.integer(year), year = ifelse((year >= 50), year + 1900,
year + 2000),) %>%
  arrange(year, month) %>%
  mutate(.data = ., month = month.name[as.integer(month)], day = as.integer(day)) %>% 
  select(year, month, close) 
```

Next, read and tidy the unemployment data for further merging process.
And make sure the key variables have the same name and take the same values.
```{r}
unemployement_data = 
  read_csv("Data/fivethirtyeight_datasets/unemployment.csv") %>%
  rename(year = Year) %>%
  pivot_longer(
    data = .,
    Jan:Dec,
    names_to = "month",
    values_to = "unemployment"
  ) %>%
  mutate(month = recode(month,
  'Jan' = 'January',
  'Feb' = 'February',
  'Mar' = 'March',
  'Apr' = 'April',
  'Jun' = 'June',
  'Jul' = 'July',
  'Aug' = 'August',
  'Sep' = 'September',
  'Oct' = 'October',
  'Nov' = 'November',
  'Dec' = 'December')) %>%
  mutate(.data = ., year = as.integer(year), month = as.character(month))
```

Join the datasets altogether.
```{r}
temp_merge_data =
  left_join(pols_month_data, snp_data, by = c("year" = "year", "month" = "month"))
result_ljoin_fte_data = 
  left_join(temp_merge_data, unemployement_data, by = c("year" = "year", "month" = "month"))
```

The resulting dataset using `left_join` has some `NA`s in it.

The result dataset by removing `NA`s can be computed:
```{r}
result_fte_data = 
  result_ljoin_fte_data %>%
  na.omit()
```

(Note: we can also use `merge` function after Google, as shows below:)

```{r}
result_merge_fte_data =
  merge(pols_month_data, snp_data) %>%
  merge(., unemployement_data)
```

Write a short paragraph about these datasets:

```{r echo = FALSE}
rows_pols = nrow(pols_month_data)
cols_pols = ncol(pols_month_data)
rows_snp = nrow(snp_data)
cols_snp = ncol(snp_data)
rows_unem = nrow(unemployement_data)
cols_unem = ncol(unemployement_data)

rows_fte = nrow(result_fte_data)
cols_fte = ncol(result_fte_data)
range_year_fte = max(result_fte_data$year) - min(result_fte_data$year)
```

The first dataset pols_month_data contain `r rows_pols` observations of `r cols_pols` variables related to the number of national politicians(president, governor, senator, representative) who are democratic or republican at any given time.

The second dataset snp_data contains `r rows_snp` observations of `r cols_snp` variables related to Standard & Poor's stock market index, and the `close` variable refers to the closing values of the stock index on the associated date.

The third dataset unemployment_data contains `r rows_unem` observations of `r cols_unem` variables which indicate the percentage of unemployment in every month of different associated years.

The merged dataset result_fte_data contains `r rows_fte` observations of `r cols_fte` variables, which merged the recorded variables from 3 separate datasets above, and provides a straightforward view of the whole data from multiple aspects, including key metrics like time(year/month), president, close and unemployment. The range of years recorded in this dataset is `r range_year_fte`.

```{r echo = FALSE}
avg_goppre_unemployment = 
  result_fte_data %>%
  filter(president == "gop") %>%
  pull(unemployment) %>%
  mean

avg_dempre_unemployment = 
  result_fte_data %>%
  filter(president == "dem") %>%
  pull(unemployment) %>%
  mean

avg_goppre_close = 
  result_fte_data %>%
  filter(president == "gop") %>%
  pull(close) %>%
  mean

avg_dempre_close = 
  result_fte_data %>%
  filter(president == "dem") %>%
  pull(close) %>%
  mean
```


A comparison of unemployment rate between when president is republican and democratic can be made. 

The average unemployment rate when the president is republican is `r avg_goppre_unemployment`, which is slightly higher than the average unemployment rate when the president is democratic that is `r avg_dempre_unemployment`.

And meanwhile, the close index when the president is republican is `r avg_goppre_close`, which is lower than when the president is democratic that is `r avg_dempre_close`. 

It is interesting since it looks like the economic state seems to be better when the president is democratic. But this statement is not too rigorous. To promote this finding and draw a clear conclusion, we still need additional data support and further investigation.


Thanks for your time!



